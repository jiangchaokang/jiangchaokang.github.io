<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Chaokang jiang | PhiGent Robotics</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Jiang Chaokang's personal academic homepage.">
    
    <meta name="keywords" content="Chaokang Jiang, 蒋超康,">
    
    
    <link rel="canonical" href="https://minimal-light-theme.yliu.me/"/>
    

    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/img/favicon-dark.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/img/favicon.png" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>

    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>

    
    <link rel="stylesheet" href="./assets/css/font.css">
    
    
    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="stylesheet" href="./assets/css/publications.css">
    

  </head>
  <body>
    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/avatar.png" alt="avatar" /></a>
        

        <h1>Chaokang jiang</h1>

        
        <position style="font-size:1.10rem;">CV Algorithm Engineer</position>
        <br>
        
        
        <a href="" rel="noopener"><autocolor>PhiGent Robotics</autocolor></a>
        <br>
        
        
        <email>jck98@foxmail.com</email>
        

        <br>
        <br>
        <div class="social-icons">
        
        <a style="margin: 0 5px 0 0" href="https://scholar.google.com/citations?user=6gZ8vloAAAAJ&hl=zh-CN">
          <i class="ai ai-google-scholar" style="font-size:1.2rem"></i>
        </a>  
        

        
        <a style="margin: 0 5px 0 0" href="assets/files/curriculum_vitae.pdf">
          <i class="ai ai-cv" style="font-size:1.3rem;"></i>
        </a>
        

        
        <a style="margin: 0 5px 0 0" href="https://github.com/jiangchaokang?tab=repositories">
          <i class="fab fa-github"></i>
        </a>
        

        

        
        <a style="margin: 0 0 0 0" href="https://x.com/chaokangjiang1">
          <i class="fab fa-twitter"></i>
        </a>
        
        </div>
        <br>

      </header>
      <section>

      <h2 id="about-me">About Me</h2>

<p>I was born in 1998 and currently work as a Computer Vision (CV) Algorithm Engineer at <a href="https://www.phigent.ai/">PhiGent Robotics</a> in Beijing. I obtained my Master’s degree in Engineering in 2023, during which I was jointly trained by <a href="https://www.cumt.edu.cn/">China University of Mining and Technology</a> and <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>. I was fortunate to be guided by Professor <a href="https://irmv.sjtu.edu.cn/wanghesheng">Hesheng Wang</a> and to collaborate with my senior, Dr. <a href="https://guangmingw.github.io/">Guangming Wang</a> from the University of Cambridge. At the <a href="https://irmv.sjtu.edu.cn/">IRMV Lab</a>, I participated in numerous engineering projects, such as lawn mowing robots and 4D automatic annotation, which significantly enhanced my practical engineering skills.</p>

<h2 id="research-interests">Research Interests</h2>

<ul>
  <li><strong>Computer Vision:</strong> image recognition, image generation, video captioning</li>
  <li><strong>World Models:</strong> model-based automatic driving scene generation</li>
  <li><strong>2D/3D Object Detection:</strong> pure vision, pure LiDAR and 2D-3D fusion deep learning algorithms</li>
  <li><strong>Deep Learning on Point Clouds:</strong> feature extraction, matching and fusion, graph network</li>
  <li><strong>3D Scene Flow:</strong> unsupervised learning, point cloud processing</li>
  <li><strong>LiDAR Odometry:</strong> large-scale point cloud registration, motion estimation</li>
</ul>

<h2 id="news">News</h2>

<ul>
  <li><strong>[Feb. 2024]</strong> Our paper “DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement” is accepted to <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DifFlow3D_Toward_Robust_Uncertainty-Aware_Scene_Flow_Estimation_with_Iterative_Diffusion-Based_CVPR_2024_paper.html">CVPR 2024</a>.</li>
  <li><strong>[Feb. 2024]</strong> Our paper “3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling” is accepted to <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_3DSFLabelling_Boosting_3D_Scene_Flow_Estimation_by_Pseudo_Auto-labelling_CVPR_2024_paper.html">CVPR 2024</a>.</li>
  <li><strong>[May. 2024]</strong> Our paper “MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models” is available on <a href="https://arxiv.org/abs/2405.14338">arXiv</a>.</li>
  <li><strong>[May. 2024]</strong> Our paper “NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation” is available on <a href="https://arxiv.org/abs/2405.14241">arXiv</a>.</li>
</ul>

<h2 id="publications" style="margin: 2px 0px -15px;">Publications</h2>

<div class="publications">
<ol class="bibliography">



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/NeuroGauss4D.png" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">arXiv</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2405.14241">NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation</a></div>
      <div class="author">Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, Zhujin Liang, Yi Shan, Dalong Du</div>
      <div class="periodical"><em>arXiv</em>
      </div>
    <div class="links">
       
      <a href="https://arxiv.org/pdf/2405.14241" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/jiangchaokang/NeuroGauss4D-PCI" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:KJ4AKAvj47gJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGaayC5ELzsosSoqWQ:AFWwaeYAAAAAZm2usWQvYkiwzuyUIOys1AM_hUg&amp;scisig=AFWwaeYAAAAAZm2usbhU5e_z7gWyn5kOMQIg5Lc&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Under Review</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/MAMBA4D.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">arXiv</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2405.14338">MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models</a></div>
      <div class="author">Jiuming Liu, Jinru Han, Lihao Liu, Angelica I Aviles-Rivero, Chaokang Jiang, Zhe Liu, Hesheng Wang</div>
      <div class="periodical"><em>arXiv</em>
      </div>
    <div class="links">
       
      <a href="https://arxiv.org/pdf/2405.14338" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/MAMBA4D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:WrzUY_XeCMcJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGaayC5ELzsosSr_5A:AFWwaeYAAAAAZm2t55DjL7oaLzmVOZMbNS09xzc&amp;scisig=AFWwaeYAAAAAZm2t5_EUqKyt4C1uUPVDFjvY3Rw&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Under Review</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/3DSFLabelling.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">CVPR 2024</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_3DSFLabelling_Boosting_3D_Scene_Flow_Estimation_by_Pseudo_Auto-labelling_CVPR_2024_paper.pdf">3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling</a></div>
      <div class="author">Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, Zhujin Liang, Yi Shan, Dalong Du</div>
      <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</em>
      </div>
    <div class="links">
       
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_3DSFLabelling_Boosting_3D_Scene_Flow_Estimation_by_Pseudo_Auto-labelling_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/jiangchaokang/3DSFLabelling" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
       
      <a href="https://jiangchaokang.github.io/3DSFLabelling-Page/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/DifFlow3D.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">CVPR 2024</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_DifFlow3D_Toward_Robust_Uncertainty-Aware_Scene_Flow_Estimation_with_Iterative_Diffusion-Based_CVPR_2024_paper.pdf">DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement</a></div>
      <div class="author">Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, Hesheng Wang</div>
      <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</em>
      </div>
    <div class="links">
       
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_DifFlow3D_Toward_Robust_Uncertainty-Aware_Scene_Flow_Estimation_with_Iterative_Diffusion-Based_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/DifFlow3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:j0TtQfGkk-MJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGaayC5ELzsosSs9FU:AFWwaeYAAAAAZm2q7FU15YFzVXt8G8heh_0c6Dw&amp;scisig=AFWwaeYAAAAAZm2q7MfyVnbdo_cnZiKoZKunrKM&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/RegFormer.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">ICCV 2023</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.pdf">RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration</a></div>
      <div class="author">Jiuming Liu, Guangming Wang, Chaokang Jiang, Zhe Liu, Hesheng Wang</div>
      <div class="periodical"><em>IEEE/CVF International Conference on Computer Vision (ICCV), 2023</em>
      </div>
    <div class="links">
       
      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/RegFormer" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/translo.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">AAAI 2023</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=6gZ8vloAAAAJ&amp;citation_for_view=6gZ8vloAAAAJ:W7OEmFMy1HYC">TransLO: A Window-Based Masked Point Transformer Framework for Large-Scale LiDAR Odometry</a></div>
      <div class="author">Jiuming Liu, Guangming Wang, Chaokang Jiang, Zhe Liu, Hesheng Wang</div>
      <div class="periodical"><em>AAAI Conference on Artificial Intelligence 2023</em>
      </div>
    <div class="links">
       
      <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=6gZ8vloAAAAJ&amp;citation_for_view=6gZ8vloAAAAJ:W7OEmFMy1HYC" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/TransLO" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:JljxJ6XFUHIJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClGaayC5ELzsosSuVxE:AFWwaeYAAAAAZm2oTxG6yWcOQ6xX3Q-4n6T8vCI&amp;scisig=AFWwaeYAAAAAZm2oT47mxYGjz3FOA_c7pxunmZs&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/ICRA2023_FFPANet.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">arXiv</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2209.07419">FFPA-Net: Efficient feature fusion with projection awareness for 3D object detection</a></div>
      <div class="author">Chaokang Jiang, Guangming Wang, Jinxing Wu, Yanzi Miao, Hesheng Wang</div>
      <div class="periodical"><em>arXiv</em>
      </div>
    <div class="links">
       
      <a href="https://arxiv.org/pdf/2209.07419" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/DongZhouGu/arxiv-daily/issues/341" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:SuO9VirALQoJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClHEWZj2ELzsosSN9Os:AFWwaeYAAAAAZm2L7OtB_M12x0DaifKLL6X0oKw&amp;scisig=AFWwaeYAAAAAZm2L7O7wFdrj0akdWJpBp3vHd7k&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Under Review</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/Vslam_PseudoLidar.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">TIM</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://ieeexplore.ieee.org/abstract/document/10251537">Pseudo-LiDAR for Visual Odometry</a></div>
      <div class="author">Yanzi Miao, Huiying Deng, Chaokang Jiang, Zhiheng Feng, Xinrui Wu, Guangming Wang, Hesheng Wang</div>
      <div class="periodical"><em>IEEE Transactions on Instrumentation and Measurement</em>
      </div>
    <div class="links">
       
      <a href="https://ieeexplore.ieee.org/abstract/document/10251537" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/Pseudo-LiDAR-for-Visual-Odometry" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:0EMOLOUvIscJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClHEWZj2ELzsosSMSks:AFWwaeYAAAAAZm2KUkueEkSZuo699DWihxZcbyg&amp;scisig=AFWwaeYAAAAAZm2KUqCBMK2rtpbZIojDN1XXF1I&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/psfnet.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">arXiv</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2209.04945">Unsupervised Learning of 3D Scene Flow with 3D Odometry Assistance</a></div>
      <div class="author">Guangming Wang, Zhiheng Feng, Chaokang Jiang, Hesheng Wang</div>
      <div class="periodical"><em>arXiv</em>
      </div>
    <div class="links">
       
      <a href="https://arxiv.org/pdf/2209.04945" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/fzhiheng/PSFNet" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:9rqR_C9N8UkJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClHEWZj2ELzsosSBWHQ:AFWwaeYAAAAAZm2HQHRQXikuHL64_Im6TWmVxVs&amp;scisig=AFWwaeYAAAAAZm2HQKRUCPKAxOxY-rXd6L4rqes&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Under Review</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/Scene_Flow_on_Pseudo_Point.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">TII</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9905954">3-D Scene Flow Estimation on Pseudo-LiDAR: Bridging the Gap on Estimating Point Motion</a></div>
      <div class="author">Chaokang Jiang; Guangming Wang; Yanzi Miao; Hesheng Wang</div>
      <div class="periodical"><em>IEEE Transactions on Industrial Informatics June 2023</em>
      </div>
    <div class="links">
       
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9905954" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/3DSF-PL" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
       
      <a href="https://ieeexplore.ieee.org/abstract/document/9905954" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:ZbbT1pES-V4J:scholar.google.com/&amp;output=citation&amp;scisdr=ClFqk38mELzsosSExGM:AFWwaeYAAAAAZm2C3GOnf0BIqy8QibRBoEb3RzY&amp;scisig=AFWwaeYAAAAAZm2C3JW_YBFXNxDbeZRo_HZl4os&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/paper/sfgan.gif" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
     
    <abbr class="badge">AIS</abbr>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202100197">SFGAN: Unsupervised Generative Adversarial Learning of 3D Scene Flow from the 3D Scene Self</a></div>
      <div class="author">Guangming Wang, Chaokang Jiang, Zehang Shen, Yanzi Miao, Hesheng Wang* (*Corresponding authors)</div>
      <div class="periodical"><em>Advanced Intelligent Systems</em>
      </div>
    <div class="links">
       
      <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202100197" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/IRMVLab/SFGAN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
       
      <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fdmc6Xj1WTMJ:scholar.google.com/&amp;output=citation&amp;scisdr=ClFqk38mELzsosR9Q9c:AFWwaeYAAAAAZm17W9f4uyNY80WEvP8mmDv750Y&amp;scisig=AFWwaeYAAAAAZm17W46vM-KHHI6BBe3m8WBKB18&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      
       
      <strong> <i style="color:#e74d3c">Published</i></strong>
      
      
    </div>
  </div>
</div>
</li>

<br />



</ol>
</div>

<h2 id="projects">Projects</h2>

<div class="project">
  <h3 class="project-title">Autonomous Intelligent Lawn Mower Robot</h3>
  <p class="project-subtitle">Collaboration Project between [SJTU IRMV](https://irmv.sjtu.edu.cn/) &amp; [Positec Technology](http://www.positecgroup.com.cn/)</p>
  
  <div class="project-images">
    
    <div class="project-image">
      <img src="/assets/img/projects/Ob_on_slope1.gif" alt="Lawn mower going up and downhill" />
      <div class="image-description">Lawn mower going up and downhill</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/Up_down_Slope.gif" alt="Lawn mower going up and downhill" />
      <div class="image-description">Lawn mower going up and downhill</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/Obstacle_Detection.gif" alt="Forward and backward 3D obstacle detection" />
      <div class="image-description">3D obstacle detection</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/lawn_ai_fusion.gif" alt="2d-3d fusion 3D obstacle detection" />
      <div class="image-description">2D detection and 3D clustering fusion</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/lawn_fusion.gif" alt="2d-3d fusion 3D obstacle detection" />
      <div class="image-description">2D segmentation and 3D clustering fusion</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/angui.gif" alt="Safety 2DBEV detection" />
      <div class="image-description">2D bird's eye view detection for safety</div>
    </div>
    
  </div>

  <p class="project-date">2021-09 ~ 2022-11</p>
  
  <h4>Responsibilities:</h4>
  <ul class="project-responsibilities">
    
    <li>Developed slope detection for lawn mowers using radar and depth camera, including point cloud clustering, segmentation, fitting, and detection techniques</li>
    
    <li>Developed 3D obstacle detection using point cloud and depth camera on the TX2 platform for various complex scenarios in flat and grassy environments</li>
    
    <li>Designed color texture feature encoding methods and efficient fusion with dense 3D point cloud features</li>
    
  </ul>
  
  <p class="project-description">The lawn mower robot project encompasses AI vision obstacle classification and recognition, multi-sensor offline and online calibration, visual-inertial odometry, global pose estimation based on multi-sensor fusion, obstacle detection techniques using LiDAR and depth cameras, and multi-sensor fusion for 3D obstacle detection.</p>
  
  <div class="project-links">
    
    <a href="http://www.positecgroup.com.cn/company/index.aspx?key=company&amp;id=5&amp;type=%e7%a7%91%e6%8a%80%e5%88%9b%e6%96%b0" target="_blank">Related News</a>
    
  </div>
</div>

<div class="project">
  <h3 class="project-title">Multimodal Fusion Technology in Autonomous Intelligent Lawn Mowing Robots</h3>
  <p class="project-subtitle">Collaboration Project between [SJTU IRMV](https://irmv.sjtu.edu.cn/) &amp; [Positec Technology](http://www.positecgroup.com.cn/)</p>
  
  <div class="project-images">
    
    <div class="project-image">
      <img src="/assets/img/projects/ffpanet.gif" alt="FFPA-Net Efficient Feature Fusion with Projection Awareness for 3D Object Detection" />
      <div class="image-description">FFPA-Net Efficient Feature Fusion with Projection Awareness for 3D Object Detection</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/soft_hard.gif" alt="The Main Network Structures of The Bi-modality Fusion Pipeline S&amp;H-Fusion" />
      <div class="image-description">The Main Network Structures of The Bi-modality Fusion Pipeline S&amp;H-Fusion</div>
    </div>
    
  </div>

  <p class="project-date">2021-09 ~ 2023-03</p>
  
  <h4>Responsibilities:</h4>
  <ul class="project-responsibilities">
    
    <li>A new data pre-processing method is proposed to achieve more efficient fusion of the different signal features. The indexes between different sensor signals are established in advance and stored in a map, while synchronized sampling provides fast and accurate query correspondence for feature fusion.</li>
    
    <li>Multiple methods are explored to achieve cross-modal feature fusion more reasonably and efficiently, including soft query weights with perceiving the Euclidean distance of bimodal features, and fusion modules based on dual attention correlating the geometric features and texture features of the scene.</li>
    
    <li>A bi-modality feature fusion module with both hard and soft components is proposed, which guides the network to refine more accurate 3D positions and orientations of objects in the second stage. The proposed method achieves advanced performance on the nuScenes dataset, especially demonstrating powerful performance for small object detection with degraded image quality and objects with few LiDAR signals.</li>
    
  </ul>
  
  <p class="project-description">This is a preliminary research project aimed at exploring data-driven methods for image and LiDAR fusion. We focused on two 2D-3D fusion approaches to address the high latency and low accuracy of current 3D obstacle detection models. Our goal is to accelerate the practical application of these models.</p>
  
  <div class="project-links">
    
    <a href="https://arxiv.org/pdf/2209.07419" target="_blank">FFPA-Net</a>
    
    <a href="assets/files/Soft_and_Hard_Associations_in_Bi_modality_Fusion_Are_All_You_Need.pdf" target="_blank">Soft and Hard Associations in Bi-modality Fusion Are All You Need</a>
    
  </div>
</div>

<div class="project">
  <h3 class="project-title">Integrated Neural Network for Perception, Planning and Decision-making</h3>
  <p class="project-subtitle">Independent Innovation Joint Fund Project of the Future Laboratory of the Second Aerospace Academy</p>
  
  <div class="project-images">
    
    <div class="project-image">
      <img src="/assets/img/projects/simulation_exploration.gif" alt="Comprehensive demonstration of reconnaissance and strike missions in a simulated environment" />
      <div class="image-description">Comprehensive demonstration of reconnaissance and strike missions in a simulated environment</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/slam_demo.gif" alt="Integrated Neural Network Technology Research for Perception, Planning, and Decision-making" />
      <div class="image-description">Integrated Neural Network Technology Research for Perception, Planning, and Decision-making</div>
    </div>
    
    <div class="project-image">
      <img src="/assets/img/projects/auto_driving2.gif" alt="Monocular Visual SLAM Real-time Dense Map Construction" />
      <div class="image-description">Monocular Visual SLAM Real-time Dense Map Construction</div>
    </div>
    
  </div>

  <p class="project-date">2021-08 ~ 2022-10</p>
  
  <h4>Responsibilities:</h4>
  <ul class="project-responsibilities">
    
    <li>Developed perception tasks in simulation environments by integrating the Webots robot simulator with deep learning for 2D tracking and detection.</li>
    
    <li>Deployed depth estimation and mapping in SLAM models, and 3D semantic segmentation models.</li>
    
    <li>Deployed real-time dense mapping for monocular visual SLAM on ROS.</li>
    
  </ul>
  
  <p class="project-description">The project focuses on designing an integrated reinforcement learning network encompassing perception, planning, and decision-making. The perception module includes depth estimation, semantic segmentation, odometry estimation, loop closure detection, dense mapping, and object detection and tracking.</p>
  
  <div class="project-links">
    
  </div>
</div>

<h2 id="services">Services</h2>

<h4 style="margin:0 10px 0;">Conference Reviewers</h4>

<ul style="margin:0 0 5px;">
  <li><a href="https://cvpr.thecvf.com/"><autocolor>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</autocolor></a></li>
  <li><a href="https://iros2024-abudhabi.org/"><autocolor>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024</autocolor></a></li>
  <li><a href="https://nips.cc/Conferences/2024"><autocolor>Conference on Neural Information Processing Systems (NIPS) 2024</autocolor></a></li>
</ul>

<h4 style="margin:0 10px 0;">Journal Reviewers</h4>

<ul style="margin:0 0 20px;">
  <li><a href="https://ieee-itss.org/pub/t-iv/"><autocolor>IEEE Transactions on Intelligent Vehicles (T-IV)</autocolor></a></li>
  <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860"><autocolor>IEEE Transactions on Robotics (TRO)</autocolor></a></li>
</ul>


      <br>

      
      <p><small>Powered by Jekyll and <a href="https://github.com/yaoyao-liu/minimal-light" target="_blank" rel="noopener">Minimal Light</a> theme.</small></p>
      

      </section>
      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
