- title: Autonomous Intelligent Lawn Mower Robot (2021-09 ~ 2022-11)
  subtitle: Collaboration Project between [SJTU IRMV](https://irmv.sjtu.edu.cn/) & [Positec Technology](http://www.positecgroup.com.cn/)
  images:
    - src: assets/img/projects/Ob_on_slope1.gif
      alt: Lawn mower going up and downhill
      description: Lawn mower going up and downhill
    - src: assets/img/projects/Up_down_Slope.gif
      alt: Lawn mower going up and downhill
      description: Lawn mower going up and downhill
    - src: assets/img/projects/Obstacle_Detection.gif
      alt: Forward and backward 3D obstacle detection
      description: 3D obstacle detection
    - src: assets/img/projects/lawn_ai_fusion.gif
      alt: 2d-3d fusion 3D obstacle detection
      description: 2D detection and 3D clustering fusion
    - src: assets/img/projects/lawn_fusion.gif
      alt: 2d-3d fusion 3D obstacle detection
      description: 2D segmentation and 3D clustering fusion
    - src: assets/img/projects/angui.gif
      alt: Safety 2DBEV detection
      description: 2D bird's eye view detection for safety
  # date: 2021-09 ~ 2022-11
  responsibilities:
    - Developed slope detection for lawn mowers using radar and depth camera, including point cloud clustering, segmentation, fitting, and detection techniques
    - Developed 3D obstacle detection using point cloud and depth camera on the TX2 platform for various complex scenarios in flat and grassy environments
    - Designed color texture feature encoding methods and efficient fusion with dense 3D point cloud features
  description: The lawn mower robot project encompasses AI vision obstacle classification and recognition, multi-sensor offline and online calibration, visual-inertial odometry, global pose estimation based on multi-sensor fusion, obstacle detection techniques using LiDAR and depth cameras, and multi-sensor fusion for 3D obstacle detection.
  links:
    - title: Related News
      url: http://www.positecgroup.com.cn/company/index.aspx?key=company&id=5&type=%e7%a7%91%e6%8a%80%e5%88%9b%e6%96%b0


- title: Multimodal Fusion Technology in Autonomous Intelligent Lawn Mowing Robots (2021-09 ~ 2023-03)
  subtitle: Collaboration Project between [SJTU IRMV](https://irmv.sjtu.edu.cn/) & [Positec Technology](http://www.positecgroup.com.cn/)
  images:
    - src: assets/img/projects/ffpanet.gif
      alt: FFPA-Net Efficient Feature Fusion with Projection Awareness for 3D Object Detection
      description: FFPA-Net Efficient Feature Fusion with Projection Awareness for 3D Object Detection
    - src: assets/img/projects/soft_hard.gif
      alt: The Main Network Structures of The Bi-modality Fusion Pipeline S&H-Fusion
      description: The Main Network Structures of The Bi-modality Fusion Pipeline S&H-Fusion

  # date: 2021-09 ~ 2023-03
  responsibilities:
    - A new data pre-processing method is proposed to achieve more efficient fusion of the different signal features. The indexes between different sensor signals are established in advance and stored in a map, while synchronized sampling provides fast and accurate query correspondence for feature fusion.
    - Multiple methods are explored to achieve cross-modal feature fusion more reasonably and efficiently, including soft query weights with perceiving the Euclidean distance of bimodal features, and fusion modules based on dual attention correlating the geometric features and texture features of the scene.
    - A bi-modality feature fusion module with both hard and soft components is proposed, which guides the network to refine more accurate 3D positions and orientations of objects in the second stage. The proposed method achieves advanced performance on the nuScenes dataset, especially demonstrating powerful performance for small object detection with degraded image quality and objects with few LiDAR signals.
  description: This is a preliminary research project aimed at exploring data-driven methods for image and LiDAR fusion. We focused on two 2D-3D fusion approaches to address the high latency and low accuracy of current 3D obstacle detection models. Our goal is to accelerate the practical application of these models.
  links:
    - title: FFPA-Net
      url: https://arxiv.org/pdf/2209.07419
    - title: Soft and Hard Associations in Bi-modality Fusion Are All You Need
      url: assets/files/Soft_and_Hard_Associations_in_Bi_modality_Fusion_Are_All_You_Need.pdf


- title: Integrated Network for Perception, Planning and Decision-making (2021-08 ~ 2022-10)
  subtitle: Independent Innovation Joint Fund Project of [The Future Laboratory of The Second Aerospace Academy](https://m.spacechina.com/)
  images:
    - src: assets/img/projects/simulation_exploration.gif
      alt: Comprehensive demonstration of reconnaissance and strike missions in a simulated environment
      description: Comprehensive demonstration of reconnaissance and strike missions in a simulated environment
    - src: assets/img/projects/slam_demo.gif
      alt: Integrated Neural Network Technology Research for Perception, Planning, and Decision-making
      description: Integrated Neural Network Technology Research for Perception, Planning, and Decision-making
    - src: assets/img/projects/auto_driving2.gif
      alt: Monocular Visual SLAM Real-time Dense Map Construction
      description: Monocular Visual SLAM Real-time Dense Map Construction
    - src: assets/img/paper/Vslam_PseudoLidar.gif
      alt: Monocular Visual SLAM Real-time Dense Map Construction
      description: Monocular Visual SLAM Real-time Dense Map Construction

  # date: 2021-08 ~ 2022-10
  responsibilities:
    - Developed perception tasks in simulation environments by integrating the Webots robot simulator with deep learning for 2D tracking and detection.
    - Deployed depth estimation and mapping in SLAM models, and 3D semantic segmentation models.
    - Deployed real-time dense mapping for monocular visual SLAM on ROS.
  description: The project focuses on designing an integrated reinforcement learning network encompassing perception, planning, and decision-making. The perception module includes depth estimation, semantic segmentation, odometry estimation, loop closure detection, dense mapping, and object detection and tracking.


- title: Video Offline 4D Automatic Labelling (2022-11 ~ 2023-04)
  subtitle: Collaboration Project between [Hozonauto](https://www.hozonauto.com/) and [SJTU IRMV](https://irmv.sjtu.edu.cn/)
  images:
    - src: assets/img/projects/4DLabelling.png
      alt: The Basic Framework of 4D Automatic Labelling Scheme
      description: The Basic Framework of 4D Automatic Labelling Scheme
    - src: assets/img/projects/4DLabelling2.png
      alt: Automatic Labelling Scheme for Ground Static Elements
      description: Automatic Labelling Scheme for Ground Static Elements

  # date: 
  responsibilities:
    - Perception team leader, responsible for reporting, summarizing, and controlling the phased progress of each sub project. Timely follow up and assist in resolving technical issues encountered.
  description: Imitating Tesla's 4D automatic labeling solution, the process includes the following parts, input signal processing, standard scene perception tasks, ground element reconstruction and labeling, static scene reconstruction and 3D dynamic object labeling, overall 4D automatic labeling summary, badcase simulation.


- title: Road Preview - Disparity Estimation (2023-05 ~ 2023-11)
  subtitle: Collaboration Project with [BYD](https://chejiahao.autohome.com.cn/info/13952185)
  images:
    - src: assets/img/projects/NDR.PNG
      alt: Road preview disparity estimation Network 
      description: Road preview disparity estimation using optimized neural networks
    - src: assets/img/projects/mp4/depth_vis.gif
      alt: Visualization of road preview depth estimation
      description: Visualization of road preview depth estimation
  links:
    - title: Road Preview Disparity Estimation Web Demo
      url: assets/html/Road_preview_disparity.html

  # date: 2023-06 ~ 2023-09
  responsibilities:
    - Optimized the NDR-Ne pseudo disparity estimation model, improving data clarity, preprocessing, and augmentation.
    - Enhanced the compressed network model for better efficiency, accuracy, and robustness.
    - Deployed the model on the TDA4-AL board, addressing various bad cases and improving overall model robustness.

  description: This project focuses on road preview and disparity estimation tasks, including improving pseudo disparity estimation models, optimizing neural networks for real-time performance, and deploying robust solutions on embedded platforms.


- title: Road Segmentation and Preview (2023-05 ~ 2024-03)
  subtitle: Collaboration Project with [BYD](https://chejiahao.autohome.com.cn/info/13952185)
  images:
    - src: assets/img/projects/seg/seg_case4.PNG
      alt: Road segmentation case example 1
      description: Road segmentation for small-scale objects like speed bumps and manhole covers
    - src: assets/img/projects/seg/seg_case3.PNG
      alt: Road segmentation case example 2
      description: Handling challenging scenarios like shadowed and wet roads
    - src: assets/img/projects/seg/model_arch.PNG
      alt: Neural network architecture for road segmentation
      description: Optimized neural network for road segmentation and preview tasks
    - src: assets/img/projects/seg/badcase_vis_waterstains.gif
      alt: Road segmentation visualization
      description: Visualization of road segmentation results on test data
  links:
    - title: Road Segmentation Demo
      url: assets/html/Road_preview_Seg.html

  responsibilities:
    - Designed and optimized the segmentation model to handle small-scale objects (e.g., speed bumps) in large-scale road environments.
    - Improved feature extraction and fusion for handling texture-similar backgrounds and objects in challenging lighting or occlusion scenarios.
    - Integrated advanced loss functions like OhemCrossEntropy, Lovasz, and RMI Loss to improve segmentation accuracy and boundary precision.
    - Deployed the segmentation model on the TDA4-AL platform, achieving real-time performance and robustness in practical scenarios.
    - Collaborated with teams to address challenging cases (e.g., shadows, water stains, and complex lighting), significantly improving model reliability.
    - Conducted quantization-aware training (QAT) to optimize the model for deployment on embedded systems without sacrificing performance.

  description: 
    This project focuses on intelligent road segmentation and preview tasks, leveraging cutting-edge neural networks to handle challenges such as small object detection, complex backgrounds, and adverse lighting conditions. The project involved end-to-end optimization, from model design and training to deployment on embedded platforms, ensuring real-time performance and robustness. Key features include advanced loss functions, quantization-aware training, and integration with BYD's intelligent systems for enhanced vehicle control and safety.



- title: 3D Motion Estimation - Scene Flow and Deployment (2023-08 ~ 2023-12)
  subtitle: Collaboration Project with [PhiGent Robotics](https://www.phigent.ai/en/)
  images:
    - src: assets/img/projects/occ3dflow/Auto-Flow.PNG
      alt: Auto-labeling pipeline for 3D motion estimation
      description: Automated pipeline for generating 3D scene flow labels
    - src: assets/img/projects/occ3dflow/e2.PNG
      alt: Experiment results on public datasets
      description: Performance evaluation of the 3D scene flow estimation network
  links:
    - title: 3D Motion Estimation Web Demo
      url: assets/html/occflow.html

  responsibilities:
    - Built a high-quality 3D scene flow training dataset using multi-source data to improve model generalization.
    - Designed and maintained two efficient 3D scene flow estimation networks, optimizing for large-scale point cloud data and complex operators.
    - Developed an automatic annotation framework to generate consistent, high-accuracy labels, significantly reducing manual labeling efforts.
    - Conducted extensive experiments on public datasets, demonstrating superior accuracy and computational efficiency compared to baseline methods.
    - Converted models to ONNX format and optimized them for deployment on NVIDIA Orin (TensorRT) and Horizon J6E (Horizon SDK).

  description: 
    This project focuses on building robust 3D motion estimation solutions, including creating advanced 3D scene flow estimation networks, developing an auto-labeling framework, and deploying the models on embedded hardware platforms. The work demonstrates significant improvements in accuracy, efficiency, and real-time performance over traditional methods while addressing challenges in operator compatibility and dataset availability.



- title: 4D Auto-Labeling and 3D Pure LiDAR Detection (2024-05 ~ 2023-11)
  subtitle: Internal R&D Project on VRU Recognition and 3D Detection
  images:
    - src: assets/img/projects/Auto_label_3DDet/pipeline.PNG
      alt: 4D Auto-Labeling System Pipeline
      description: Pipeline for 4D auto-labeling system to enhance VRU detection
    - src: assets/img/projects/Auto_label_3DDet/Copy-Paste1.PNG
      alt: Data Augmentation with 3D Object Copy-Paste
      description: Example of 3D object data augmentation to improve model robustness
    - src: assets/img/projects/Auto_label_3DDet/loss1.PNG
      alt: Geometric Alignment Loss Use Case
      description: Use case for optimizing detection via geometric alignment loss
    - src: assets/img/projects/Auto_label_3DDet/vis1_pure_lidar_det.png
      alt: Auto-labeling Visualization Example
      description: Construction of a pipeline for mining badcase using Pure LiDAR 3D Object Detection Model
  links:
    - title: Learn More About the Project
      url: assets/html/4D_Auto_label_3DDet.html

  responsibilities:
    - Enhanced VRU detection recall rates through:
        - Data-level optimizations: 3D object augmentation using Copy-Paste techniques, incorporation of LiDAR reflectivity features, and expansion of manually labeled VRU and trailer datasets.
        - Model-level optimizations: Improved BEV (Bird's Eye View) resolution and adapted network structures for varying perception ranges.
        - Loss function optimizations: Introduced Geometric Alignment Loss to refine detection box precision.
    - Conducted extensive experiments on internal datasets, showcasing significant improvements in detection accuracy and robustness.
    - Designed a 3D pure LiDAR detection model, focusing on:
        - Dataset adaptation and pipeline creation to ensure high-quality annotations.
        - Iterative model architecture and parameter optimization to enhance performance.
        - Identifying and fixing failure cases by prioritizing and resolving critical issues.
    - Established a continuous improvement cycle for bad-case analysis and validation of optimization strategies.

  description: 
    This project integrates a 4D auto-labeling system with an efficient 3D pure LiDAR detection model to enhance VRU (Vulnerable Road User) recognition and detection precision. It addresses challenges in data annotation, improves network robustness through data/model optimization, and deploys cutting-edge solutions for real-world applications. The work emphasizes innovation in data augmentation, loss function design, and system-level analysis to achieve superior detection accuracy and recall rates.


- title: End-to-End Autonomous Driving (LiDAR + 11V Fusion) Project (2024-06 ~ 2023-09)
  subtitle: Collaboration Project with [lantu](https://mp.weixin.qq.com/s/c0uLSZqOiKBg7yZARQeD5w)
  images:
    - src: assets/img/projects/11v_e2e/vis1.PNG
      alt: End-to-End Autonomous Driving Visualization
      description: Visualization of end-to-end autonomous driving system with LiDAR and 11V fusion
    - src: assets/img/projects/11v_e2e/arch1.PNG
      alt: End-to-End Autonomous Driving Architecture 1
      description: System architecture for LiDAR and multi-camera fusion in autonomous driving
  links:
    - title: End-to-End Autonomous Driving Video Visualization
      url: assets/html/11v_e2e.html

  responsibilities:
    - Developed pipelines for data reading, processing, and format conversion, ensuring seamless integration of LiDAR and camera data.
    - Implemented LiDAR motion distortion correction using global pose transformation matrices and timestamp synchronization.
    - Designed and optimized LiDAR-camera fusion for obstacle detection, leveraging sparse anchors and map instances to enhance multi-modal feature aggregation and improve perception accuracy.

  description: This project aims to achieve an end-to-end autonomous driving system by fusing LiDAR data with 11 multi-view cameras. The system outputs high-precision perception, prediction, planning, and control information, enabling enhanced autonomous driving performance in complex environments.


- title: Controlled Data Generation for Traffic Scenarios (2023-05 ~ 2023-11)
  subtitle: Collaboration Project with [BYD](https://chejiahao.autohome.com.cn/info/13952185)
  images:
    - src: assets/img/projects/contrl_gen_video/ours_pipeline.png
      alt: Generation Pipeline
      description: Controlled data generation pipeline with ControlNet and Stable Diffusion
    - src: assets/img/projects/contrl_gen_video/rain_4fisheye.gif
      alt: Rainy Scene Generation
      description: Rainy scene generation using Our autoregressive video generation framework
  links:
    - title: Detailed introduction of AIGC model
      url: assets/html/contrl_gen_video.html

  responsibilities:
    - Developed a **single-frame generation module** supporting multi-view (4 fisheye, 7V, 11V) data creation and editing with integrated 3D bounding boxes and map labels, optimizing algorithms for better efficiency and completeness.
    - Designed a **video generation module** using autoregressive modeling to produce high-quality continuous frames with temporal consistency, integrating DDIM for faster inference and improved visual quality.
    - Supported conditional video generation based on text prompts or offline BEV traffic flow, ensuring realistic and diverse driving scenarios.
    - Leveraged ControlNet to align temporal conditions and latent features, enabling controlled multi-view surround video generation in a single pass.
    - Achieved automated generation for multi-view data, supporting 4 fisheye, 7V, and 11V formats, with enhanced temporal coherence and cross-view consistency.

  description: This project focuses on generating diverse and temporally consistent traffic scenario videos using advanced generative models such as LLM-enhanced world models and latent diffusion models. The system supports multi-view video generation under various conditions (e.g., weather, time of day) and ensures adherence to traffic rules, providing valuable data for autonomous driving and traffic planning.